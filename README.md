# Flash-Attention-Implementation

Implementation of Flash-Attention (both forward and backward) with PyTorch, LibTorch, CUDA, and Triton

## Geting Started

* PyTorch
  ```shell
  cd flashattn/pytorch
  python flashattn.py
  ```
* LibTorch
  ```shell
  cd flashattn/libtorch
  python test.py
  ```
* CUDA
  - [ ] TODO
* Triton
  - [ ] TODO
