# Flash-Attention-Implementation

Implementation of Flash-Attention (both forward and backward) with PyTorch, LibTorch, CUDA, and Triton

## Geting Started

* PyTorch
  ```shell
  python flashattn/python/flashattn.py
  ```
* LibTorch
  ```shell
  python flashattn/libtorch/test.py
  ```
* CUDA
  - [ ] TODO
* Triton
  - [ ] TODO